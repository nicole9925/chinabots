{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'org'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-355-5c8a27c813f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhanzidentifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdayofmonth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrom_unixtime\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munix_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'org'"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkFiles\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *\n",
    "import pyspark.sql.functions as f\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import hanzidentifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"/Users/nicole/Desktop/projects/china bots/chinabots/data/tweets1.csv\", \"/Users/nicole/Desktop/projects/china bots/chinabots/data/tweets2.csv\", \"/Users/nicole/Desktop/projects/china bots/chinabots/data/tweets3_1.csv\", \"/Users/nicole/Desktop/projects/china bots/chinabots/data/tweets3_2.csv\", \"/Users/nicole/Desktop/projects/china bots/chinabots/data/tweets3_3.csv\", \"/Users/nicole/Desktop/projects/china bots/chinabots/data/users1.csv\", \"/Users/nicole/Desktop/projects/china bots/chinabots/data/users2.csv\", \"/Users/nicole/Desktop/projects/china bots/chinabots/data/users3.csv\"]\n",
    "for path in paths:\n",
    "    sc.addFile(path)\n",
    "\n",
    "tweets1 = sqlContext.read.csv(SparkFiles.get(paths[0]), header=True, inferSchema=True)\n",
    "tweets2 = sqlContext.read.csv(SparkFiles.get(paths[1]), header=True, inferSchema=True)\n",
    "tweets31 = sqlContext.read.csv(SparkFiles.get(paths[2]), header=True, inferSchema=True)\n",
    "tweets32 = sqlContext.read.csv(SparkFiles.get(paths[3]), header=True, inferSchema=True)\n",
    "tweets33 = sqlContext.read.csv(SparkFiles.get(paths[4]), header=True, inferSchema=True)\n",
    "\n",
    "users1 = sqlContext.read.csv(SparkFiles.get(paths[5]), header=True, inferSchema=True)\n",
    "users2 = sqlContext.read.csv(SparkFiles.get(paths[6]), header=True, inferSchema=True)\n",
    "users3 = sqlContext.read.csv(SparkFiles.get(paths[7]), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = [tweets1, tweets2, tweets31, tweets32, tweets33]\n",
    "tweets = reduce(DataFrame.unionAll, tweets_df)\n",
    "\n",
    "users_df = [users1, users2, users3]\n",
    "users = reduce(DataFrame.unionAll, users_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertColumn(df, names, newType):\n",
    "    for name in names: \n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Clean dataframes ####\n",
    "tweets_int_features = [\"userid\"]\n",
    "tweets = convertColumn(tweets, tweets_int_features, IntegerType())\n",
    "# drop duplicate albums\n",
    "tweets = reduce(DataFrame.drop, [\"follower_count\", \"following_count\"], tweets)\n",
    "\n",
    "users_int_features = [\"userid\", \"follower_count\", \"following_count\"]\n",
    "users = convertColumn(users, users_int_features, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqlContext.createDataFrame(users.rdd.takeSample(False, 50, seed=None)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_random_user(users, tweets):\n",
    "    \"\"\"\n",
    "    Takes in two pyspark dataframes, finds random user, returns two python dataframe: one with user info, one with user tweets\n",
    "    \"\"\"\n",
    "    frame = None\n",
    "    while frame==None:\n",
    "        random = users.rdd.takeSample(False, 1, seed=None)\n",
    "        display_name = random[0].__getattr__(\"user_display_name\")\n",
    "        if display_name == None:\n",
    "            continue\n",
    "        else:\n",
    "            frame = tweets.filter((f.col(\"user_display_name\") == display_name))\n",
    "            if frame.count() == 0:\n",
    "                frame = None\n",
    "    pd_frame = frame.toPandas()\n",
    "    pd_random = pd.DataFrame(random[0].asDict(), index=[0])\n",
    "    return pd_random, pd_frame, random[0], frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_user, pd_tweet, spark_user, spark_tweet = find_random_user(users, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_chinese_tweet_pd(user, user_metadata):\n",
    "    \"\"\"\n",
    "    Takes in the user and tweets dataframes and returns information on users language changes if they switched to Chinese\n",
    "    \"\"\"\n",
    "    user[\"tweet_time\"] = pd.to_datetime(user[\"tweet_time\"], errors=\"coerce\")\n",
    "    user[\"chinese\"] = user[\"tweet_text\"].apply(hanzidentifier.has_chinese)\n",
    "    user = (user.loc[~user[\"tweet_time\"].isnull()])\n",
    "    chinese_df = (user.groupby(\"chinese\")[\"tweet_time\"]\n",
    "                  .apply(list)\n",
    "                  .to_frame()\n",
    "                  .reset_index())\n",
    "    falses = chinese_df.loc[chinese_df[\"chinese\"] == False]\n",
    "    trues = chinese_df.loc[(chinese_df[\"chinese\"] == True)]\n",
    "    if (falses.shape[0] == 0) or (trues.shape[0] == 0):\n",
    "        return None\n",
    "    \n",
    "    non_chinese = falses[\"tweet_time\"].item()\n",
    "    non_chinese.sort(reverse=False)\n",
    "    num_non = (len(non_chinese))\n",
    "    \n",
    "    chinese = trues[\"tweet_time\"].item()\n",
    "    chinese.sort(reverse=False)\n",
    "    num_chinese = (len(chinese))\n",
    "    \n",
    "    account_creation = user_metadata[\"account_creation_date\"].iloc[0]\n",
    "    \n",
    "    if non_chinese[0] > chinese[0]:\n",
    "        return None\n",
    "    \n",
    "    return {\"first_chinese\": chinese[0],  \"last_chinese\": chinese[-1], \"num_chinese\": num_chinese, \"first_non_chinese\": non_chinese[-1], \"last_non_chinese\": non_chinese[0], \"num_non_chinese\": num_non}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_chinese_tweet_spark(user, tweets, user_pd, tweets_pd):\n",
    "    \"\"\"\n",
    "    Takes in spark user and tweets dataframes and returns information on users language changes if they switched to Chinese\n",
    "    \"\"\"\n",
    "#     tweets.groupBy(\"tweet_language\").filter((f.col()))\n",
    "\n",
    "    tweets_sub = tweets.select(\"tweet_language\", \"tweet_time\")\n",
    "    tweets_sub = tweets_sub.withColumn('tweet_time', \n",
    "                   f.to_date(f.unix_timestamp(f.col('tweet_time'), 'MM-dd-yyyy HH:mm').cast(\"timestamp\")))\n",
    "    tweets_sub = tweets_sub.groupBy(\"tweet_language\").agg(f.min(\"tweet_time\").alias(\"earliest\"))\n",
    "    tweets_pd[\"tweet_\"]\n",
    "    return tweets_sub.count()\n",
    "#     return tweets.withColumn(\"test3\", f.expr(\"to_date(value, format)\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find_first_chinese_tweet_spark(spark_user, spark_tweet)\n",
    "find_first_chinese_tweet_spark(spark_user, spark_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_tweet.withColumn(\"chinese\", hanzidentifier.has_chinese(spark_tweet.tweet_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tweetid: string (nullable = true)\n",
      " |-- userid: integer (nullable = true)\n",
      " |-- user_display_name: string (nullable = true)\n",
      " |-- user_screen_name: string (nullable = true)\n",
      " |-- user_reported_location: string (nullable = true)\n",
      " |-- user_profile_description: string (nullable = true)\n",
      " |-- user_profile_url: string (nullable = true)\n",
      " |-- account_creation_date: string (nullable = true)\n",
      " |-- account_language: string (nullable = true)\n",
      " |-- tweet_language: string (nullable = true)\n",
      " |-- tweet_text: string (nullable = true)\n",
      " |-- tweet_time: string (nullable = true)\n",
      " |-- tweet_client_name: string (nullable = true)\n",
      " |-- in_reply_to_userid: string (nullable = true)\n",
      " |-- in_reply_to_tweetid: string (nullable = true)\n",
      " |-- quoted_tweet_tweetid: string (nullable = true)\n",
      " |-- is_retweet: string (nullable = true)\n",
      " |-- retweet_userid: string (nullable = true)\n",
      " |-- retweet_tweetid: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- quote_count: string (nullable = true)\n",
      " |-- reply_count: string (nullable = true)\n",
      " |-- like_count: string (nullable = true)\n",
      " |-- retweet_count: string (nullable = true)\n",
      " |-- hashtags: string (nullable = true)\n",
      " |-- urls: string (nullable = true)\n",
      " |-- user_mentions: string (nullable = true)\n",
      " |-- poll_choices: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_tweet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterations = 500\n",
    "# switch_date = []\n",
    "# i=0\n",
    "# while i < iterations:\n",
    "#     user, user_tweets = find_random_user(users, tweets)\n",
    "#     diction = find_first_chinese_tweet(user_tweets, user)\n",
    "#     if diction != None:\n",
    "#         i+=1\n",
    "#         switch_date.append(diction[\"first_chinese\"])\n",
    "#         print(diction[\"first_chinese\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2019-06-25 07:06\n",
       "1      2019-06-25 07:13\n",
       "2      2019-07-17 07:08\n",
       "3                  None\n",
       "4      2019-07-14 06:44\n",
       "             ...       \n",
       "159                None\n",
       "160                None\n",
       "161    2019-06-19 06:36\n",
       "162    2019-05-09 23:41\n",
       "163    2019-07-08 09:12\n",
       "Name: tweet_time, Length: 164, dtype: object"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_tweet[\"tweet_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
